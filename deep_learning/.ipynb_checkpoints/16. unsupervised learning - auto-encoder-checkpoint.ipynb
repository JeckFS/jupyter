{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时学习Encoder和decode。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/311.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA只有一个hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/312.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/313.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 在编码和解码过程中$W_i$不一定非得是对称的。\n",
    "* initialize by RBM layer-by-layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/314.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/315.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/316.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vector space model: 将每篇文章表示为空间中的一个点，把要查询的词汇也变成空间中的一个点，计算这个词汇与每篇文章的inner product。如何将一篇文章表示为vector呢，因为如何表示会影响到最终的结果。\n",
    "常见方法有：\n",
    "* bag-of-word<br>\n",
    "  vector 的大小就是所有单词的个数，文章中出现某个词汇就标1，没有就标0<br>\n",
    "  缺点是不能够将语义(semantic)考虑在内。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/317.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "应用：通过某个单词或字符串检索文章，比如输入query这个词，它会先进行编码，看它落在哪个位置(文章)，根据位置信息，将所在文章检索出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/318.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面这种搜寻效果并不好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/319.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to sovle? 将图片变为一个code，利用code搜寻。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/320.png'/>\n",
    "<img src='images/321.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "auto-encoder的一个应用：用于pre-training上，比如参数的初始化。如先把input转变为1000维，然后再转回487维。这需要做一个很强的正则化项，应为784维可能“原封不动”放入1000维，然后再转为784维，这样什么都没有学到。比如可以对这1000维的输出做L1的regularization。<br>\n",
    "然后将参数$W^1$固定，将所有的数据都转化为1000维的数据，每一笔数据都再经过->1000维->1000维的auto-encoder过程。将学到的参数$W^2$保存下来。再接着由1000维->500维->1000维的auto-encoder过程，将$W^3$保存下来。这个$W^1,W^2,W^3$就等于训练这个network的初始化过程。然后再随机初始化500维->10维的参数。然后再用反向传播将参数微调一遍(find-tune)。因为现在不用做pre-training也能训练的很好，所以这种方法一般用在只有少量的labeled data上，用大量的unlabeled data去初始化参数，利用labeled data去微调参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/322.png'/>\n",
    "<img src='images/323.png'/>\n",
    "<img src='images/324.png'/>\n",
    "<img src='images/325.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#1907d8' size=4>变种：</font>\n",
    "* de-noising auto-encoder：可以学习如何去掉噪点。\n",
    "* contractive auto-encoder：对code加一个constraint，就是当input有变化的时候，对code的影响是被minimize的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/326.png'/>\n",
    "<img src='images/327.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/328.png'/>\n",
    "<img src='images/329.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：deep belief network与deep neoral netWork不是一个东西。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/330.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unpooling的过程就是记录选取最大值的位置，unpooling时按位置归位，其它位置不变。unpooling的方法不唯一，也有只是简单的repeat the values就是将扩散后的位置也置为最大值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/331.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/332.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/333.png'/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
