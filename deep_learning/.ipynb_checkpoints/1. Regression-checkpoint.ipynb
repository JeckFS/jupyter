{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 应用："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测股票，自动驾驶，推荐系统。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/9.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/10.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/11.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若宝可梦的种类也是一个很重要的因素，不同种类的宝可梦对应不同的线性模型。如下：\n",
    "<img src='images/12.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仍然可以将所有的线性模型转换为一个线性模型，如下：\n",
    "<img src='images/15.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "期望小的参数，因为参数越小，引起的y的变化越小，模型曲线更平滑。噪点对于平滑的曲线影响较小。$\\lambda$是需要手动调节的参数。\n",
    "<img src='images/16.png'/>\n",
    "正则化后，在训练集上的error可能会增大(loss function考虑了权重，而error的计算是不包含权重的)，有趣的是在测试集上的error可能会变小。<br>\n",
    "<font color='#f40c7a' size=3>思考为什么正则化不考虑参数$b$？</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#f0110c' size=3>模型参数越多越容易过拟合</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/17.png'/>\n",
    "learning rate太小收敛速度慢，太大会跳过最优解或者收敛很快，实作中需要将Loss关于$\\Delta \\theta$的图像画出来。观察那个学习率较好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Learning Rates(自动调节学习率)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/18.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/19.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/20.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/21.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#f40c7a' size=3>一个小小的疑问：</font>\n",
    "$g^t$越大而$\\sqrt{\\sum_{i=0}^t (g^i)^2}$越大。一个想增大步伐一个想减小步伐。\n",
    "<img src='images/22.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个直观的解释：降低较大“反差”带来的影响。\n",
    "<img src='images/23.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在只考虑一个参数的时候，参数的微分值越大，则证明它离最低点越远，\n",
    "<img src='images/24.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "离minima的远近与微分值成正比，在跨参数情况下是不对的。a的微分值比较小，但离minima较远，c的微分值较大，但离minima较近。\n",
    "<img src='images/25.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最好的step是$$\\frac{|first derivative|}{second derivative}$$\n",
    "<img src='images/26.png'/>\n",
    "\n",
    "在a点其一次微分小，二次微分较小；在c点一次微分较大，二次也较大。一次与二次的比值更能说明a点与c点距离minima的远近。\n",
    "<img src='images/27.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述分析与Adagrad的关系：<br>\n",
    "&emsp;&emsp;其实是用$\\sqrt{\\sum_{i=0}^t {(g^i)^2}}$来近似二次微分值。\n",
    "<img src='images/28.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Densent与Gradient Descent的不同在于， SGD只是随机的取出一个example就更新参数。收敛较快。\n",
    "<img src='images/29.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似于数据标准化。\n",
    "<img src='images/30.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/31.png'/>\n",
    "未Feature scaling之前，需要不同的学习率才能得到较好结果，上楼练速度也较慢。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling的一种做法：\n",
    "<img src='images/32.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/33.png'/>\n",
    "<font color='#f40c7a' size=3>如何从红色圈内找到最低点呢？</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/35.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/36.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/37.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当$(\\Delta \\theta_1, \\Delta \\theta_2)$与$(u,v)$反向且达到圆圈边缘时，$L(\\theta)$最小。于是有:\n",
    "$$\n",
    "\\left [\n",
    "\\begin{matrix}\n",
    "    \\Delta \\theta_1 \\\\\n",
    "    \\Delta \\theta_2\n",
    "\\end{matrix}\n",
    "\\right ]=-\\eta\n",
    "\\left [\n",
    "\\begin{matrix}\n",
    "    u\\\\\n",
    "    v\n",
    "\\end{matrix}\n",
    "\\right ]\n",
    "$$\n",
    "<img src='images/38.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过演变，它就是gradient Descent。前提是圈圈(步长)够小。\n",
    "<img src='images/39.png'/>\n",
    "当然如果考虑了二次式(如牛顿法)，圈圈就可以大一些，那个不等式仍可能成立。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent的局限性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/40.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
