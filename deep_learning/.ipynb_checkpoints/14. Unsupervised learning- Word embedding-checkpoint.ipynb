{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 典型的单词编码方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1-of-N encoding**只是将单词编码，并不能包含单词间的信息。如cat与dog是同一类等等。针对此，提出**Word Class**，即将单词分类，用类的属性表示类别内的word，但类别之间可能存在一定关系也无法表示出来，比如class1与class2。针对此，提出**word embedding**。它把每一个word都映射到一个高维空间中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/280.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入单词，输出Word enbedding。他是一个无监督的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/281.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据上下文来分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/282.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两种方法去分析上下文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/283.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction-based的思想是通过前一个词来预测下一个词出现的概率。输入一个1-of-N encoding of the $W_{i-1}$，经过权重的运算产生的向量$Z$可以来代表这个输入的1-of-N encoding。本例中由$Z$确定的空间如下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/284.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/285.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仅仅只由前一个词汇来预测下一个词汇，这是很难的。How to solve? 可以有前n个词汇来预测第n+1这个词汇。不同输入词汇的相同位置的到同一个neoron的权重参数是相同的(下图中用相同颜色的箭头表示)。Why to do this? 1. 不同的单词序列得到的结果是不同的输出。 2. 减少参数量，另外随着单词增多，就不会需要更多的参数，因为所有单词的权重是公用的。<br><br>每个1-of-N encoding of the word $W_{i-2}$乘以$W$就会得到对应的word embedding。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/286.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何保证$W_i$是一样的呢？参数矩阵给予相同的初始值，<img src='images/287.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to train this network? \n",
    "<img src='images/288.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction-based model的变形：\n",
    "<img src='images/289.png'/>\n",
    "有趣的是这个neural network只有一层hidden layer。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现Word与Word之间的关系："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/290.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "属于包含与被包含关系的词相减之后，会落在同一个区域。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/291.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用Word vector的概念，进行推论可以得到："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/292.png'/>\n",
    "运用此种方法可以退出另一个对应的未知单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用Word embedding甚至可以做到类似翻译的功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/293.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding用于图像分类：<br>\n",
    "假设我们知道某些Word vector的分类情况，当出现一个未知的图像时，往往可以得到比较好的结果。比如输入一张猫的图片。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/294.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document Embedding：将一个document变为一个vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/295.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方法是：将Document变为一个Bag-of-word，利用auto encode 学习出Semantic Embedding。仅仅用bag-of-word去描述一个Document是不够的。因为它不能考虑到词汇的顺序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/296.png'/>\n",
    "<img src='images/297.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：\n",
    "<img src='images/298.png'/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
