{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从语言中提取出对应的Slot："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/181.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/182.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**局限：**不同的语境中同一个词具有不同的含义，这在通常的Feedfoward network中，相同的输入会差生一样的输出，这便不能区分语境。改进的做法是在Feedforward network中增加“**记忆功能**”，让网络能够考虑其他单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/183.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**每个隐藏层**的会存储以前单词输入后的输出结果，该结果参与下一个单词的输入。memory有一个初始值。不同的序列产生不同的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/184.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每输入一个词，都会考虑memory的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/185.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jordan Network的memory中存储的是前一个单词的最终输出结果。实验证明这种会得到较好的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/186.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**双向RNN**：双向指的是单词的读入顺序可以是逆向的。也就是同时训练一个正向RNN，一个逆向RNN，两个网络的hidden layer都接入一个output layer，产生最后的输出。这样的输出结果会考虑的范围是整个句子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/187.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/188.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input Gate的输入信号$z_i$可以看做是对输入$z$的控制信号：当$f(z_i)$=0的时候，$g(z)f(z_i)=0$输入无效；当$f(z_i)=1$时，$g(z)f(z_i)=g(z)$输入有效。<br>\n",
    "Forget Gate的输入信号$z_f$可以看做是是否考虑原先的memory值：当$f(z_f)=0$时，$cf(z_f)=0$视为不考虑原先的memory值；当$f(z_f)=1$时，$cf(z_f)=c$视为考虑原先的memory值。<br>\n",
    "同理，output gate也可以看做是对输出的控制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/189.png'/>\n",
    "<img src='images/190.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/191.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于输入：可以看做只考虑$x_1$的输入。<br>\n",
    "关于input gate：可以看做当$x_2$输出较大的时候，input gate打开，较小的死后，input gate关闭<br>\n",
    "forget gate，output gate同理。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/192.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN与原始的神经网络的相似处：每个LITM看做一个neoron，每个neoron需要4个输入。图中的每条彩色线都有一个weight，它与input做运算生成neoron的一个输入。因此RNN的参数数量是原始神经网络的4倍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/193.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个输入$x^t$都会乘上一个矩阵得到neoron的不同输入向量。$c^{t-1}$是memory中的值组成的向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/194.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z,z^f,z^o,z^i$是矩阵，图中的乘是逐元素的乘。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/195.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/196.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际中，neoron不光考虑input还会考虑上一阶段的memory向量和输出$y^t$($h^t$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/197.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多层的LSTM样子："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/198.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM中的Cost："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将每个单词丢入RNN后，计算输出结果与对应Slot vector的crossentropy，这多个crossentropy的和就是要minimize的对象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/199.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN中的反向传播：BPTT，会考虑到时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/200.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/201.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/202.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直观理解上述现象：<br>\n",
    "同一个参数在不同的时间点会被反复使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/203.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM会拿掉一写很平坦的区域，一些断崖式的地方还会存在，所以learning rate一定要设小一点。\n",
    "1. 原始的RNN中的前一个memory会被当前的覆盖掉，而LSTM是input与memory中内容的**和**。\n",
    "2. 如果某个参数会对memory中的值存在影响，那么这个影响 never disappears unless forget gate is closed.\n",
    "\n",
    "\n",
    "应确保大部分时间forget gate是开启的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/204.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overfitting很严重的话，可以尝试GRU。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他解决gradient vanish问题的技术：Clockwise RNN; Structurally Constrained Recurrent Network(SCRN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/205.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN并不一定是一个输入对应一个输出，也可以做更复杂的事情。<br>\n",
    "RNN的更多应用："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 多个输入一个输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 情感分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/206.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 主题提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/207.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 多个输入对应多个输出(output is shorter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 语音辨识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/208.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "叠字处理："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/209.png'/>\n",
    "<img src='images/210.png'/>\n",
    "<img src='images/211.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 多个输入对应多个输出(no limitations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 翻译"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/212.png'/>\n",
    "<img src='images/213.png'/>\n",
    "<img src='images/214.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 其他"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/215.png'/>\n",
    "<img src='images/216.png'/>\n",
    "<img src='images/217.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
